{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "1           0.638645    0.637558              1              0.75                      0.0075776     \n",
      "2           0.637558    0.636471              0.75           0.75                      0.0173064     \n",
      "3           0.636471    0.635382              0.75           0.75                      0.0268685     \n",
      "4           0.635382    0.634293              0.75           0.75                      0.0367492     \n",
      "5           0.634293    0.633203              0.75           0.75                      0.0438332     \n",
      "6           0.633203    0.632112              0.75           0.75                      0.0525252     \n",
      "7           0.632112    0.63102               0.75           0.75                      0.0613022     \n",
      "8           0.63102     0.629927              0.75           0.75                      0.0688115     \n",
      "9           0.629927    0.628834              0.75           0.75                      0.0753263     \n",
      "10          0.628834    0.62774               0.75           0.75                      0.0842679     \n",
      "11          0.62774     0.626646              0.75           0.75                      0.0910007     \n",
      "12          0.626646    0.62555               0.75           0.75                      0.0977369     \n",
      "13          0.62555     0.624454              0.75           0.75                      0.105647      \n",
      "14          0.624454    0.623357              0.75           0.75                      0.112046      \n",
      "15          0.623357    0.622259              0.75           0.75                      0.119569      \n",
      "16          0.622259    0.621161              0.75           0.75                      0.12618       \n",
      "17          0.621161    0.620062              0.75           0.75                      0.133735      \n",
      "18          0.620062    0.618962              0.75           0.75                      0.14027       \n",
      "19          0.618962    0.617861              0.75           0.75                      0.147438      \n",
      "20          0.617861    0.61676               0.75           0.75                      0.154937      \n",
      "21          0.61676     0.615675              0.75           0.75                      0.162135      \n",
      "22          0.615675    0.61457               0.75           0.75                      0.169452      \n",
      "23          0.61457     0.613474              0.75           0.75                      0.176427      \n",
      "24          0.613474    0.612384              0.75           0.75                      0.183695      \n",
      "25          0.612384    0.611292              0.75           0.75                      0.190906      \n",
      "26          0.611292    0.610201              0.75           0.75                      0.208323      \n",
      "27          0.610201    0.609108              0.75           0.75                      0.219786      \n",
      "28          0.609108    0.608014              0.75           0.75                      0.22972       \n",
      "29          0.608014    0.60692               0.75           0.75                      0.239063      \n",
      "30          0.60692     0.605825              0.75           0.75                      0.246856      \n",
      "31          0.605825    0.604729              0.75           0.75                      0.256351      \n",
      "32          0.604729    0.603632              0.75           0.75                      0.263807      \n",
      "33          0.603632    0.602535              0.75           0.75                      0.27156       \n",
      "34          0.602535    0.601437              0.75           0.75                      0.27989       \n",
      "35          0.601437    0.600367              0.75           0.75                      0.287163      \n",
      "36          0.600367    0.599271              0.75           0.75                      0.294809      \n",
      "37          0.59927     0.598164              0.75           0.75                      0.302614      \n",
      "38          0.598164    0.59708               0.75           0.75                      0.310199      \n",
      "39          0.59708     0.595995              0.75           0.75                      0.318489      \n",
      "40          0.595995    0.594908              0.75           0.75                      0.326251      \n",
      "41          0.594908    0.59382               0.75           0.75                      0.33508       \n",
      "42          0.59382     0.592732              0.75           0.75                      0.342973      \n",
      "43          0.592732    0.591642              0.75           0.75                      0.351224      \n",
      "44          0.591642    0.590551              0.75           0.75                      0.359383      \n",
      "45          0.590551    0.589465              0.75           0.75                      0.367958      \n",
      "46          0.589465    0.588394              0.75           0.75                      0.375839      \n",
      "47          0.588394    0.587311              0.75           0.75                      0.385037      \n",
      "48          0.587311    0.586218              0.75           0.75                      0.393132      \n",
      "49          0.586218    0.585142              0.75           0.75                      0.405509      \n",
      "50          0.585142    0.584065              0.75           0.75                      0.41897       \n",
      "51          0.584065    0.582986              0.75           0.75                      0.429919      \n",
      "52          0.582986    0.581906              0.75           0.75                      0.440522      \n",
      "53          0.581906    0.580824              0.75           0.75                      0.448875      \n",
      "54          0.580824    0.579742              0.75           0.75                      0.457422      \n",
      "55          0.579742    0.578682              0.75           0.75                      0.465689      \n",
      "56          0.578682    0.577605              0.75           0.75                      0.4747        \n",
      "57          0.577605    0.576523              0.75           0.75                      0.482935      \n",
      "58          0.576523    0.575459              0.75           0.75                      0.492025      \n",
      "59          0.575459    0.574392              0.75           0.75                      0.500402      \n",
      "60          0.574392    0.573324              0.75           0.75                      0.508979      \n",
      "61          0.573324    0.572254              0.75           0.75                      0.518046      \n",
      "62          0.572254    0.571182              0.75           0.75                      0.526445      \n",
      "63          0.571182    0.570124              0.75           0.75                      0.535276      \n",
      "64          0.570124    0.569056              0.75           0.75                      0.545282      \n",
      "65          0.569056    0.567998              0.75           0.75                      0.554107      \n",
      "66          0.567998    0.566946              0.75           0.75                      0.56297       \n",
      "67          0.566946    0.565891              0.75           0.75                      0.571848      \n",
      "68          0.565891    0.564834              0.75           0.75                      0.580847      \n",
      "69          0.564834    0.563774              0.75           0.75                      0.594346      \n",
      "70          0.563774    0.562713              0.75           0.75                      0.605115      \n",
      "71          0.562713    0.561677              0.75           0.75                      0.61973       \n",
      "72          0.561677    0.560628              0.75           0.75                      0.628794      \n",
      "73          0.560628    0.559564              0.75           0.75                      0.638993      \n",
      "74          0.559564    0.558523              0.75           0.75                      0.648497      \n",
      "75          0.558523    0.55748               0.75           0.75                      0.657233      \n",
      "76          0.55748     0.556435              0.75           0.75                      0.666117      \n",
      "77          0.556435    0.555398              0.75           0.75                      0.675734      \n",
      "78          0.555398    0.554356              0.75           0.75                      0.685267      \n",
      "79          0.554356    0.553321              0.75           0.75                      0.694718      \n",
      "80          0.553321    0.552297              0.75           0.75                      0.704426      \n",
      "81          0.552297    0.551259              0.75           0.75                      0.713585      \n",
      "82          0.551259    0.550228              0.75           0.75                      0.723934      \n",
      "83          0.550228    0.549204              0.75           0.75                      0.733752      \n",
      "84          0.549204    0.548169              0.75           0.75                      0.743751      \n",
      "85          0.548169    0.547146              0.75           0.75                      0.75307       \n",
      "86          0.547146    0.54613               0.75           0.75                      0.764735      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87          0.54613     0.545115              0.75           0.75                      0.776812      \n",
      "88          0.545115    0.544099              0.75           0.75                      0.792446      \n",
      "89          0.544099    0.543079              0.75           0.75                      0.804785      \n",
      "90          0.543079    0.542075              0.75           0.75                      0.822963      \n",
      "91          0.542075    0.541062              0.75           0.75                      0.837985      \n",
      "92          0.541062    0.540056              0.75           0.75                      0.853835      \n",
      "93          0.540056    0.539058              0.75           0.75                      0.871592      \n",
      "94          0.539058    0.538055              0.75           0.75                      0.889792      \n",
      "95          0.538055    0.537048              0.75           0.75                      0.903913      \n",
      "96          0.537048    0.536053              0.75           0.75                      0.919994      \n",
      "97          0.536053    0.535058              0.75           0.75                      0.935166      \n",
      "98          0.535058    0.534065              0.75           0.75                      0.948105      \n",
      "99          0.534065    0.533081              0.75           0.75                      0.96222       \n",
      "100         0.533081    0.532091              0.75           0.75                      0.975994      \n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainer.initializers as I\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "class MyChain(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(MyChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1=L.Linear(2,3)\n",
    "            self.l2=L.Linear(3,2)\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        y = self.l2(h1)\n",
    "        return y\n",
    "    \n",
    "epoch = 100\n",
    "batchsize = 4\n",
    "\n",
    "trainx = np.array(([0,0],[0,1],[1,0],[1,1]), dtype=np.float32)\n",
    "trainy = np.array([0,1,1,1], dtype=np.int32)\n",
    "train = chainer.datasets.TupleDataset(trainx, trainy)\n",
    "test = chainer.datasets.TupleDataset(trainx, trainy)\n",
    "\n",
    "model = L.Classifier(MyChain(), lossfun=F.softmax_cross_entropy)\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "test_iter = chainer.iterators.SerialIterator(test, batchsize, repeat = False, shuffle=False) \n",
    "\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'))\n",
    "\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))\n",
    "trainer.extend(extensions.PrintReport(['epoch','main/loss','validation/main/loss','main/accuracy','validation/main/accuracy','elapsed_time']))\n",
    "\n",
    "trainer.run()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
